{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import skimage.io as io\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as ioimage_stack\n",
    "import glob\n",
    "import uuid\n",
    "\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir = \"E:/Datasets/coral-classifier/few-v-rest\"\n",
    "augmented_data_dir = \"E:/Datasets/coral-classifier/augmented_dataset-v2\"\n",
    "CLASS_NAMES = os.listdir(data_dir)\n",
    "dirs = [data_dir + \"/\" + c for c in CLASS_NAMES ]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "\n",
    "def to_jpg_from(dateset_dir, classes, from_extension=\"jfif\"):\n",
    "    for cl in classes:\n",
    "        for file in tqdm(glob.glob(f\"{dateset_dir}/{cl}/*.{from_extension}\")):\n",
    "            im = Image.open(file)\n",
    "            rgb_im = im.convert('RGB')\n",
    "            rgb_im.save(file.replace(\".\"+from_extension, \"_\" + str(uuid.uuid4()) + \".jpg\"), quality=100)\n",
    "\n",
    "\n",
    "def rename_files(dateset_dir, classes, extension=\"jpg\"):\n",
    "    for cl in classes:\n",
    "        for i, file in tqdm(enumerate(glob.glob(f\"{dateset_dir}/{cl}/*.{extension}\"))):\n",
    "            num = str(i).rjust(6, \"0\")\n",
    "            cl_name = cl.replace(\" \", \"\")\n",
    "            os.rename(file, f\"{dateset_dir}/{cl}/{cl_name}_{num}.{extension}\")\n",
    "            \n",
    "    \n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "        plt.axis('off')\n",
    "        \n",
    "        \n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == CLASS_NAMES\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class 'lps acanthastrea'\n",
      "processing class 'lps chalice'\n",
      "processing class 'lps euphyllia'\n",
      "processing class 'other'\n",
      "processing class 'sps acropora'\n",
      "processing class 'sps montipora'\n",
      "processing class 'zoa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "to_jpg_from(data_dir, CLASS_NAMES, from_extension=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2061it [00:00, 4700.00it/s]\n",
      "2353it [00:00, 4701.06it/s]\n",
      "1910it [00:00, 4610.14it/s]\n",
      "3735it [00:00, 5117.44it/s]\n",
      "6865it [00:01, 3974.72it/s]\n",
      "2173it [00:01, 1685.59it/s]\n",
      "2542it [00:00, 5738.15it/s]\n"
     ]
    }
   ],
   "source": [
    "rename_files(data_dir, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test and Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_images = {}\n",
    "\n",
    "for cl, data_path in zip(CLASS_NAMES, dirs):\n",
    "    class_images[cl] = np.array(io.collection.glob(data_path + \"/*.jpg\"))\n",
    "\n",
    "counts = [len(value) for key, value in class_images.items()]\n",
    "min_count = min(counts)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sets = {}\n",
    "for key, value in class_images.items():\n",
    "    X = value\n",
    "    y = [key for item in value]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1755, random_state=1)\n",
    "        \n",
    "    sets[key + \"_train\"] = X_train\n",
    "    sets[key + \"_test\"] = X_test\n",
    "    sets[key + \"_val\"] = X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lps acanthastrea_train -> 1443\n",
      "lps acanthastrea_test -> 310\n",
      "lps acanthastrea_val -> 308\n",
      "lps chalice_train -> 1649\n",
      "lps chalice_test -> 353\n",
      "lps chalice_val -> 351\n",
      "lps euphyllia_train -> 1338\n",
      "lps euphyllia_test -> 287\n",
      "lps euphyllia_val -> 285\n",
      "other_train -> 2616\n",
      "other_test -> 561\n",
      "other_val -> 558\n",
      "sps acropora_train -> 4810\n",
      "sps acropora_test -> 1030\n",
      "sps acropora_val -> 1025\n",
      "sps montipora_train -> 1522\n",
      "sps montipora_test -> 326\n",
      "sps montipora_val -> 325\n",
      "zoa_train -> 1780\n",
      "zoa_test -> 382\n",
      "zoa_val -> 380\n"
     ]
    }
   ],
   "source": [
    "for key, value in sets.items():\n",
    "    print(key, \"->\", len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move files to train, test and val dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E:/Datasets/coral-classifier/train_ready_dataset' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/lps acanthastrea' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/lps chalice' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/lps euphyllia' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/other' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/sps acropora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/sps montipora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/train/zoa' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/lps acanthastrea' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/lps chalice' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/lps euphyllia' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/other' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/sps acropora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/sps montipora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/test/zoa' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/lps acanthastrea' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/lps chalice' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/lps euphyllia' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/other' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/sps acropora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/sps montipora' already exists\n",
      "'E:/Datasets/coral-classifier/train_ready_dataset/val/zoa' already exists\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"E:/Datasets/coral-classifier/train_ready_dataset\"\n",
    "\n",
    "set_folders = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "# make sure that dirs exist\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except Exception:\n",
    "    print(f\"'{output_dir}' already exists\")\n",
    "\n",
    "for set_folder in set_folders:\n",
    "    p = f\"{output_dir}/{set_folder}\"\n",
    "    try:\n",
    "        os.mkdir(p)\n",
    "    except Exception:\n",
    "        print(f\"'{p}' already exists\")\n",
    "    for cl in CLASS_NAMES:\n",
    "        p = f\"{output_dir}/{set_folder}/{cl}\"\n",
    "        try:\n",
    "            os.mkdir(p)\n",
    "        except Exception:\n",
    "            print(f\"'{p}' already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1443/1443 [00:00<00:00, 1779.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 310/310 [00:02<00:00, 111.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 308/308 [00:02<00:00, 106.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1649/1649 [00:26<00:00, 62.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 353/353 [00:05<00:00, 67.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 351/351 [00:05<00:00, 63.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1338/1338 [00:21<00:00, 62.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 287/287 [00:04<00:00, 66.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 285/285 [00:04<00:00, 63.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2616/2616 [00:35<00:00, 74.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 561/561 [00:07<00:00, 75.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [00:07<00:00, 77.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4810/4810 [01:12<00:00, 65.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1030/1030 [00:16<00:00, 63.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1025/1025 [00:16<00:00, 63.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1522/1522 [00:13<00:00, 111.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 326/326 [00:02<00:00, 113.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 325/325 [00:02<00:00, 112.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1780/1780 [00:30<00:00, 58.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 382/382 [00:05<00:00, 66.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 380/380 [00:05<00:00, 65.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from shutil import copy\n",
    "dest_path_base = \"E:/Datasets/coral-classifier/train_ready_dataset\"\n",
    "\n",
    "\n",
    "for key, value in sets.items():\n",
    "    splitted = key.split(\"_\")\n",
    "    cl = splitted[0]\n",
    "    set_folder = splitted[1]\n",
    "    \n",
    "    for file_path in tqdm(value):\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        copy(file_path, f\"{dest_path_base}/{set_folder}/{cl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}